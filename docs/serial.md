The optimized version of code is `cloth_code_opt.cpp` and the executable is `kernel_main_opt`. Detailed usage will be mentioned in `README.md`. The related analysis is base on the performance report generated by Intel Advisor and the PAPI metrics collected in `data/main.csv` and `data/opt.csv`. The plots are generated by `plot.ipynb`.

The elapsed time of the origin and optimized version is shown in figure \ref{fig:main_opt_time}. The x-axis is the $n$ and the y-axis is the elapsed time. The elapsed time of the optimized version is lower than the origin version. The optimized version is about 2.0 times faster than the origin version. The following parts are optimized:

#### Stride of Memory Access

The origin version of code access the array with stride $n$. By switching the loop order we can achieve sequential memory access. The read/write miss rate can be reduce by this optimization.

```diff
  for(i = 0; i < n; i++) {
    for (j = 0; j < n; j++) {
-      x[j * n + i] = 0;
+      x[i * n + j] = 0;
      // ...
    }
```

Theoretically, the L1 miss rate of the optimized version should be lower than the main version. However, from figure(in /plots/plot.ipynb), we can see the L1 miss rate of the optimized version is higher than the main version, the possible reasons are:

- The calculation of approximate L1 miss rate is based on the total instructions. Not all the instructions are memory access instructions. Also, the main version might have larger proportion of computing instructions than memory access, so the actual miss of main version rate could be much bigger.
- The optimized version has less total instructions than the main version, since the optimized version reduce the times of square root calculation in \verb|eval_pef| from $O(iter\cdot n^2d^2)$ to $O(d^2)$ by lookup table(will be mentioned in the later paragraphs).

#### Iterators of Loops

The iterators can be calculated only once at the beginning of the loop. 


#### Redundant Calculations

These optimizations(**Redundant Calculations** and **Iterators of Loops**) is trivial to the overall performance, which is helpful to reduce the total instructions(See fig(instructions of opt version)), but it is a good preparation for vectorization and parallelization in the following steps.

#### Optimize Division

#### Avoid Square Root

By the analysis of Intel Advisor, we can see the `eval_pef` function is the most time consuming function. The square root operation is used within the inner loop and is called $O(iter\cdot n^2d^2)$ times. By using hotspots statistics of VTune, we can see the square root operations are expensive.

#### Lookup Table

In function `eval_pef`, the reference distance is calculted using expensive `sqrt`. We observed that `nx - dx` and `ny - dy` are both integers ranged from $0$ to `delta`. Thus a table can be calculted for looking up square root value.

```c++
void calculate_table(int delta, double sep) {
  int ii, jj;
  // malloc for table
  table = (double**)malloc((delta + 1) * sizeof(double *));
  for(ii = 0; ii < delta + 1; ii++) {
    table[ii] = (double*)malloc((delta + 1) * sizeof(double));
  }
  for (ii = 0; ii < delta + 1; ii++) {
    for (jj = ii; jj < delta + 1; jj++) {
      table[ii][jj] = table[jj][ii] = sqrt((double)(ii * ii + jj * jj)) * sep;
    }
  }
}
double query_table(int a, int b) {
  a = ((a >> 31) ^ a) - (a >> 31);
  b = ((b >> 31) ^ b) - (b >> 31);
  return table[a][b];
}
```

#### Optimize Function `eval_pef`

Swap the nested loops, and iterate the `delta` in the outer loop. Then the `rlen` can be calculate outside instead, in order to reduce the square root calculations:

```c++
for (dy = -delta; dy <= delta; dy++) 
{
  for (dx = -delta; dx <= delta; dx++) 
  {
    rlen = query_table(dx, dy);
    if (dy == 0 && dx == 0) {
      continue;
    }
    for(ny = MAX(-dy, 0); ny < MIN(n - dy, n); ny++) 
    {
      for(nx = MAX(-dx, 0); nx < MIN(n - dx, n); nx++) 
      {
        // ...
      }
    }
  }
}
```
